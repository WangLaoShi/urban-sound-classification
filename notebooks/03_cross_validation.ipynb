{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stratified cross validation\n",
    "\n",
    "The goal of this notebook is to compare the four obtained training sets to decide on which one to run a grid search to find a good model.\n",
    "\n",
    "The models tested for each dataset are default neural networks with a numebr of hidden neuros equals to two third of the input plus the output. parameters are kept default and the training last 100 epochs.\n",
    "\n",
    "Stratified cross validation is perfomed to accoutn for class imbalance in the training set, also, class weights are considered when training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import NeuralNetwork\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import class_weight\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(1)\n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "def get_cross_scores(path, neurons):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data.drop(\"class\", axis=1)\n",
    "    y = data[\"class\"]\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                      np.unique(y),\n",
    "                                                      y)\n",
    "    weights_dict = dict(zip(np.unique(y), class_weights))\n",
    "    acc=[]\n",
    "    loss=[]\n",
    "\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        net = NeuralNetwork.create_model(neurons=neurons)\n",
    "        net.fit(x.iloc[train_index], \n",
    "                y.iloc[train_index],\n",
    "                batch_size=64, \n",
    "                epochs=100, \n",
    "                verbose=0, \n",
    "                class_weight=weights_dict)\n",
    "        scores = net.evaluate(x.iloc[test_index], \n",
    "                              y.iloc[test_index], verbose=1)\n",
    "        acc.append(scores[1])\n",
    "        loss.append(scores[0])\n",
    "    \n",
    "    return {\"Accuracy\" : (np.mean(acc), np.std(acc), acc),\n",
    "            \"Loss\" : (np.mean(loss), np.std(loss), loss)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First unscaled dataset\n",
    "The first model is tested on the unscaled dataset, this has 132 features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "res_1 = get_cross_scores(\"../data/processed/initial/train_unscaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.1111\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.3026 - accuracy: 0.1111\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 54869046067200.0000 - accuracy: 0.1211\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.3009 - accuracy: 0.1144\n",
      "29/29 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.1112\n",
      "{'Accuracy': (0.11380249559879303,\n",
      "              0.0038706225007881373,\n",
      "              [0.1111111119389534,\n",
      "               0.1111111119389534,\n",
      "               0.12111110985279083,\n",
      "               0.11444444209337234,\n",
      "               0.11123470216989517]),\n",
      " 'Loss': (nan,\n",
      "          nan,\n",
      "          [nan, 2.3026485443115234, 54869046067200.0, 2.300891160964966, nan])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First scaled dataset\n",
    "The second model is tested on the scaled dataset, 132 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "res_2 = get_cross_scores(\"../data/processed/initial/train_scaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 2.0357 - accuracy: 0.5667\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.1175 - accuracy: 0.5644\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.0790 - accuracy: 0.6378\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.3364 - accuracy: 0.5478\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.3352 - accuracy: 0.5551\n",
      "{'Accuracy': (0.5743455648422241,\n",
      "              0.03242956403450645,\n",
      "              [0.5666666626930237,\n",
      "               0.5644444227218628,\n",
      "               0.6377778053283691,\n",
      "               0.5477777719497681,\n",
      "               0.5550611615180969]),\n",
      " 'Loss': (2.180751657485962,\n",
      "          0.12919648526724084,\n",
      "          [2.0357139110565186,\n",
      "           2.1174960136413574,\n",
      "           2.078993558883667,\n",
      "           2.3363943099975586,\n",
      "           2.335160493850708])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extended and scaled dataset\n",
    "This dataset has more features, 144 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "res_3 = get_cross_scores(\"../data/processed/extended/train_extended.csv\", (144, 70, 30, 10))\n",
    "pprint(res_3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 1.9134 - accuracy: 0.6267\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.9468 - accuracy: 0.6067\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.2096 - accuracy: 0.6778\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.9098 - accuracy: 0.6011\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.4097 - accuracy: 0.5273\n",
      "{'Accuracy': (0.6078949451446534,\n",
      "              0.04855248704962302,\n",
      "              [0.6266666650772095,\n",
      "               0.6066666841506958,\n",
      "               0.6777777671813965,\n",
      "               0.601111114025116,\n",
      "               0.5272524952888489]),\n",
      " 'Loss': (2.0778797388076784,\n",
      "          0.1999730871773237,\n",
      "          [1.913411021232605,\n",
      "           1.946790337562561,\n",
      "           2.209632635116577,\n",
      "           1.9098454713821411,\n",
      "           2.409719228744507])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA dataset\n",
    "This is a reduced extended scaled dataset, with 120 features found by PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "res_4 = get_cross_scores(\"../data/processed/extended/train_pca.csv\", (120, 60, 25, 10))\n",
    "pprint(res_4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 1ms/step - loss: 1.8793 - accuracy: 0.5767\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.5592 - accuracy: 0.6511\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.2078 - accuracy: 0.6822\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.8133 - accuracy: 0.6122\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.0984 - accuracy: 0.5495\n",
      "{'Accuracy': (0.6143443346023559,\n",
      "              0.04814182775076292,\n",
      "              [0.5766666531562805,\n",
      "               0.6511111259460449,\n",
      "               0.6822222471237183,\n",
      "               0.6122221946716309,\n",
      "               0.5494994521141052]),\n",
      " 'Loss': (1.9116067171096802,\n",
      "          0.22695251120073814,\n",
      "          [1.879289984703064,\n",
      "           1.5592150688171387,\n",
      "           2.2078046798706055,\n",
      "           1.8132739067077637,\n",
      "           2.098449945449829])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final decision\n",
    "\n",
    "As seen in this notebook, the PCA training set led to better performances on the cross validation, therefore it is selected to perform hyperparameter tuning."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}