{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cross validation\n",
    "\n",
    "The goal of this notebook is to compare the four obtained training sets to decide on which one to run a grid search to find a good model.\n",
    "\n",
    "The models tested for each dataset are default neural networks with a numebr of hidden neuros equals to two third of the input plus the output. parameters are kept default and the training last 100 epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import NeuralNetwork\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_cross_scores(path, neurons):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data.drop(\"class\", axis=1)\n",
    "    y = data[\"class\"]\n",
    "    \n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    acc=[]\n",
    "    loss=[]\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        net = NeuralNetwork.create_model(neurons=neurons)\n",
    "        net.fit(x.iloc[train_index], \n",
    "                y.iloc[train_index],\n",
    "                batch_size=64, \n",
    "                epochs=100, \n",
    "                verbose=0)\n",
    "        scores = net.evaluate(x.iloc[test_index], \n",
    "                              y.iloc[test_index], verbose=1)\n",
    "        acc.append(scores[1])\n",
    "        loss.append(scores[0])\n",
    "    \n",
    "    return {\"Accuracy\" : (np.mean(acc), np.std(acc), acc),\n",
    "            \"Loss\" : (np.mean(loss), np.std(loss), loss)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First unscaled dataset\n",
    "The first model is tested on the unscaled dataset, this has 132 features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "res_1 = get_cross_scores(\"../data/processed/initial/train_unscaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 2ms/step - loss: 2.2321 - accuracy: 0.0578\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3849 - accuracy: 0.0267\n",
      "15/15 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.2556\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.2520 - accuracy: 0.0489\n",
      "15/15 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.2022\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.9720 - accuracy: 0.3333\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.2974 - accuracy: 0.0089\n",
      "15/15 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.1289\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.4740 - accuracy: 0.3400\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.3872 - accuracy: 0.1782\n",
      "{'Accuracy': (0.15803959779441357,\n",
      "              0.11748412387736448,\n",
      "              [0.057777777314186096,\n",
      "               0.02666666731238365,\n",
      "               0.25555557012557983,\n",
      "               0.04888888821005821,\n",
      "               0.20222222805023193,\n",
      "               0.3333333432674408,\n",
      "               0.008888889104127884,\n",
      "               0.12888889014720917,\n",
      "               0.3400000035762787,\n",
      "               0.1781737208366394]),\n",
      " 'Loss': (nan,\n",
      "          nan,\n",
      "          [2.232120990753174,\n",
      "           2.3848912715911865,\n",
      "           nan,\n",
      "           2.25197696685791,\n",
      "           nan,\n",
      "           2.9720394611358643,\n",
      "           2.2974164485931396,\n",
      "           nan,\n",
      "           2.4739701747894287,\n",
      "           3.387160539627075])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First scaled dataset\n",
    "The second model is tested on the scaled dataset, 132 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "res_2 = get_cross_scores(\"../data/processed/initial/train_scaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 2ms/step - loss: 2.4903 - accuracy: 0.5044\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.6903 - accuracy: 0.4844\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.3521 - accuracy: 0.5333\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.2133 - accuracy: 0.5711\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.7439 - accuracy: 0.4889\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.0729 - accuracy: 0.5200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7912 - accuracy: 0.7756\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.5855 - accuracy: 0.5622\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.4809 - accuracy: 0.3889\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.8906 - accuracy: 0.5902\n",
      "{'Accuracy': (0.5419089287519455,\n",
      "              0.09469623201509875,\n",
      "              [0.504444420337677,\n",
      "               0.48444443941116333,\n",
      "               0.5333333611488342,\n",
      "               0.5711110830307007,\n",
      "               0.4888888895511627,\n",
      "               0.5199999809265137,\n",
      "               0.7755555510520935,\n",
      "               0.5622222423553467,\n",
      "               0.3888888955116272,\n",
      "               0.5902004241943359]),\n",
      " 'Loss': (2.4311001300811768,\n",
      "          0.7799554310137715,\n",
      "          [2.4903297424316406,\n",
      "           2.6902523040771484,\n",
      "           2.352134943008423,\n",
      "           2.2133467197418213,\n",
      "           3.743863582611084,\n",
      "           2.0728633403778076,\n",
      "           0.7911885976791382,\n",
      "           2.58548641204834,\n",
      "           3.4808926582336426,\n",
      "           1.8906430006027222])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extended and scaled dataset\n",
    "This dataset has more features, 144 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "res_3 = get_cross_scores(\"../data/processed/extended/train_extended.csv\", (144, 70, 30, 10))\n",
    "pprint(res_3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 1.7071 - accuracy: 0.5978\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.0002 - accuracy: 0.4644\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.6994 - accuracy: 0.6244\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.9755 - accuracy: 0.6444\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.8144 - accuracy: 0.4667\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 2.7205 - accuracy: 0.5089\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6426 - accuracy: 0.8200\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.9971 - accuracy: 0.6156\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.4562 - accuracy: 0.4378\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.2828 - accuracy: 0.5857\n",
      "{'Accuracy': (0.5765746176242829,\n",
      "              0.10803045893195957,\n",
      "              [0.597777783870697,\n",
      "               0.46444445848464966,\n",
      "               0.6244444251060486,\n",
      "               0.644444465637207,\n",
      "               0.46666666865348816,\n",
      "               0.5088889002799988,\n",
      "               0.8199999928474426,\n",
      "               0.6155555844306946,\n",
      "               0.4377777874469757,\n",
      "               0.5857461094856262]),\n",
      " 'Loss': (2.3296024084091185,\n",
      "          0.891361621883871,\n",
      "          [1.707097053527832,\n",
      "           3.000248432159424,\n",
      "           1.69943368434906,\n",
      "           1.9755271673202515,\n",
      "           3.814427375793457,\n",
      "           2.720547676086426,\n",
      "           0.6426222324371338,\n",
      "           1.9970605373382568,\n",
      "           3.4562246799468994,\n",
      "           2.2828352451324463])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA dataset\n",
    "This is a reduced extended scaled dataset, with 120 features found by PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "res_4 = get_cross_scores(\"../data/processed/extended/train_pca.csv\", (120, 60, 25, 10))\n",
    "pprint(res_4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 2.1279 - accuracy: 0.6356\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.6515 - accuracy: 0.4244\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.7323 - accuracy: 0.6733\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.5358 - accuracy: 0.6267\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 4.2802 - accuracy: 0.4356\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.7936 - accuracy: 0.6311\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.7403 - accuracy: 0.8289\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.1022 - accuracy: 0.5311\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 3.3580 - accuracy: 0.4600\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.5124 - accuracy: 0.5768\n",
      "{'Accuracy': (0.5823504090309143,\n",
      "              0.11846516258756422,\n",
      "              [0.6355555653572083,\n",
      "               0.42444443702697754,\n",
      "               0.6733333468437195,\n",
      "               0.6266666650772095,\n",
      "               0.4355555474758148,\n",
      "               0.6311110854148865,\n",
      "               0.8288888931274414,\n",
      "               0.5311111211776733,\n",
      "               0.46000000834465027,\n",
      "               0.576837420463562]),\n",
      " 'Loss': (2.583436393737793,\n",
      "          0.9924335616439395,\n",
      "          [2.127903461456299,\n",
      "           3.6515390872955322,\n",
      "           1.7323474884033203,\n",
      "           2.5358080863952637,\n",
      "           4.280186176300049,\n",
      "           1.7936289310455322,\n",
      "           0.7402594089508057,\n",
      "           3.10223388671875,\n",
      "           3.358044147491455,\n",
      "           2.512413263320923])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final decision\n",
    "\n",
    "As seen in this notebook, the PCA training set led to better performances on the cross validation, therefore it is selected to perform hyperparameter tuning."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}