{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cross validation\n",
    "\n",
    "The goal of this notebook is to compare the four obtained training sets to decide on which one to run a grid search to find a good model.\n",
    "\n",
    "The models tested for each dataset are default neural networks with a numebr of hidden neuros equals to two third of the input plus the output. parameters are kept default and the training last 100 epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.model import NeuralNetwork\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_cross_scores(path, neurons):\n",
    "    data = pd.read_csv(path)\n",
    "    x = data.drop(\"class\", axis=1)\n",
    "    y = data[\"class\"]\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    acc=[]\n",
    "    loss=[]\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        net = NeuralNetwork.create_model(neurons=neurons)\n",
    "        net.fit(x.iloc[train_index], \n",
    "                y.iloc[train_index],\n",
    "                batch_size=64, \n",
    "                epochs=100, \n",
    "                verbose=0)\n",
    "        scores = net.evaluate(x.iloc[test_index], \n",
    "                              y.iloc[test_index], verbose=1)\n",
    "        acc.append(scores[1])\n",
    "        loss.append(scores[0])\n",
    "    \n",
    "    return {\"Accuracy\" : (np.mean(acc), np.std(acc), acc),\n",
    "            \"Loss\" : (np.mean(loss), np.std(loss), loss)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First unscaled dataset\n",
    "The first model is tested on the unscaled dataset, this has 132 features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "res_1 = get_cross_scores(\"../data/processed/initial/train_unscaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0678\n",
      "29/29 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.1556\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.3502 - accuracy: 0.0089\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.2813 - accuracy: 0.0411\n",
      "29/29 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0612\n",
      "{'Accuracy': (0.06690248474478722,\n",
      "              0.04883372520743649,\n",
      "              [0.06777777522802353,\n",
      "               0.15555556118488312,\n",
      "               0.008888889104127884,\n",
      "               0.041111111640930176,\n",
      "               0.061179086565971375]),\n",
      " 'Loss': (nan, nan, [nan, nan, 2.3502118587493896, 2.2813379764556885, nan])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First scaled dataset\n",
    "The second model is tested on the scaled dataset, 132 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "res_2 = get_cross_scores(\"../data/processed/initial/train_scaled.csv\", (132, 60, 30, 10))\n",
    "pprint(res_2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 2.8165 - accuracy: 0.4667\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.2384 - accuracy: 0.5789\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 3.2964 - accuracy: 0.4444\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.9310 - accuracy: 0.6178\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.6305 - accuracy: 0.4928\n",
      "{'Accuracy': (0.5201095044612885,\n",
      "              0.06681752075511521,\n",
      "              [0.46666666865348816,\n",
      "               0.5788888931274414,\n",
      "               0.4444444477558136,\n",
      "               0.6177777647972107,\n",
      "               0.4927697479724884]),\n",
      " 'Loss': (2.582569193840027,\n",
      "          0.4710694651929501,\n",
      "          [2.8165314197540283,\n",
      "           2.2383999824523926,\n",
      "           3.2963662147521973,\n",
      "           1.9310065507888794,\n",
      "           2.6305418014526367])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extended and scaled dataset\n",
    "This dataset has more features, 144 features and Standard Scaler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "res_3 = get_cross_scores(\"../data/processed/extended/train_extended.csv\", (144, 70, 30, 10))\n",
    "pprint(res_3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 2.4795 - accuracy: 0.5056\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.9699 - accuracy: 0.6178\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 3.2567 - accuracy: 0.4656\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.5739 - accuracy: 0.6933\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 2.2995 - accuracy: 0.5106\n",
      "{'Accuracy': (0.5585579037666321,\n",
      "              0.08418217105044334,\n",
      "              [0.5055555701255798,\n",
      "               0.6177777647972107,\n",
      "               0.4655555486679077,\n",
      "               0.6933333277702332,\n",
      "               0.510567307472229]),\n",
      " 'Loss': (2.3159227848052977,\n",
      "          0.5625732692380198,\n",
      "          [2.4795339107513428,\n",
      "           1.969861626625061,\n",
      "           3.2567317485809326,\n",
      "           1.5739480257034302,\n",
      "           2.2995386123657227])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA dataset\n",
    "This is a reduced extended scaled dataset, with 120 features found by PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "res_4 = get_cross_scores(\"../data/processed/extended/train_pca.csv\", (120, 60, 25, 10))\n",
    "pprint(res_4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29/29 [==============================] - 0s 1ms/step - loss: 2.5294 - accuracy: 0.4444\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 1.7078 - accuracy: 0.6511\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 3.4708 - accuracy: 0.4800\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.4853 - accuracy: 0.7033\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 2.3463 - accuracy: 0.5250\n",
      "{'Accuracy': (0.5607833385467529,\n",
      "              0.09981858607316675,\n",
      "              [0.4444444477558136,\n",
      "               0.6511111259460449,\n",
      "               0.47999998927116394,\n",
      "               0.70333331823349,\n",
      "               0.5250278115272522]),\n",
      " 'Loss': (2.307897138595581,\n",
      "          0.6985271471207426,\n",
      "          [2.5293829441070557,\n",
      "           1.7077693939208984,\n",
      "           3.4707772731781006,\n",
      "           1.4852628707885742,\n",
      "           2.3462932109832764])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final decision\n",
    "\n",
    "As seen in this notebook, the PCA training set led to better performances on the cross validation, therefore it is selected to perform hyperparameter tuning."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}