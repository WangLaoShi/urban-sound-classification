\section{Model definition}
\label{model-definition}

This Section starts by presenting how the networks used on the initial training sets are 
structured. We proceed to give a detailed overview of the performances 
on the different training sets, lastly, the hyperparameter tuning phase
is described.

\subsection{Neural network structure}
The starting point for the neural network structure is a 
reasonable network in terms of hidden neurons to prevent over-fitting, 
indeed a high number of units in the hidden layers would end up in learning 
too much from the dataset, leading to poor performances on the test sets.

For this reason, the rule of thumb followed to decide hidden neurons quantity is the 
following: 
$$\#\mathit{hidden\; neurons} = \frac{2}{3}\big(\#\mathit{input\;neurons}
+ \#\mathit{output\;neurons}\big)$$
The next step is to decide the hidden layers number. As using the rule 
presented above gives a quite small amount of units, only two layers are considered,  
indeed, an higher quantity would mean having a real small number of neurons per layer.

Applying this rule ended up in the following architectures on the four 
different training sets, where the output layer is fixed at 10:
\begin{center}
    \begin{tabular}{ |l|r|r|r| } 
        \hline
        Training set & Input & 1st Hidden & 2nd Hidden  \\
        \hline
        132 features unscaled &  132 & 60 & 30 \\
        132 features scaled &  132 & 60 & 30 \\
        180 features scaled &  180 & 80 & 46 \\
        102 features reduced with PCA &  102 & 45 & 30 \\
        \hline
    \end{tabular}
\end{center}

To build the actual model \emph{Tensorflow} and \emph{Keras} libraries 
are used.~\cite{tensorflow}\cite{keras}

\paragraph{Starting point model}
To give reference, this is the model used on the PCA training set, mentioned at 
the end of Subsection \vref{extended-dataset}, with 
102 input features.
\begin{center}
    
    \begin{tikzpicture}[x=1.5cm, y=1cm, >=stealth]
        
        % layers
        \foreach \m/\l [count=\y] in {1,2,3,missing,4}
        \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};
        
        \foreach \m [count=\y] in {1,missing,2}
        \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (1.75,3-\y*1.75) {};
        
        \foreach \m [count=\y] in {1,missing,2}
        \node [every neuron/.try, neuron \m/.try ] (hidden2-\m) at (3.5,2-\y*1.25) {};
        
        \foreach \m [count=\y] in {1,missing,2}
        \node [every neuron/.try, neuron \m/.try ] (output-\m) at (5.25,1.5-\y) {};
        
        %   label
        \foreach \l [count=\i] in {1,2,3,{102}}
        \draw [<-] (input-\i) -- ++(-1,0)
        node [above, midway] {$I_{\l}$};
        
        \foreach \l [count=\i] in {1,45}
        \node [above] at (hidden-\i.north) {$H_{\l}$};
        
        \foreach \l [count=\i] in {1,30}
        \node [above] at (hidden2-\i.north) {$H_{\l}$};
        
        \foreach \l [count=\i] in {1,10}
        \draw [->] (output-\i) -- ++(1,0)
        node [above, midway] {$O_{\l}$};
        
        \foreach \i in {1,...,4}
        \foreach \j in {1,...,2}
        \draw [->] (input-\i) -- (hidden-\j);
        
        \foreach \i in {1,...,2}
        \foreach \j in {1,...,2}
        \draw [->] (hidden-\i) -- (hidden2-\j);
        
        \foreach \i in {1,...,2}
        \foreach \j in {1,...,2}
        \draw [->] (hidden2-\i) -- (output-\j);
        
        \foreach \l [count=\x from 0] in {Input, 1st Hidden, 2nd Hidden, Output}
        \node [align=center, above] at (\x*1.75,2) {\l \\ layer};
        
    \end{tikzpicture}
\end{center}
This last model, with the one built to predict the 180 features training set, are the ones selected for 
the hyperparameter tuning phase. 

\paragraph{Activation and loss functions}
The activation function for the input and hidden layers is a \emph{Relu}, 
typically used to prevent the \emph{vanishing gradient problem}, 
and the output one uses a \emph{Softmax} to have classification probabilities
among classes.~\cite{relu}\cite{soft}\cite{vanishing}\\
The loss used is the \emph{Sparse Categorical Crossentropy loss} as it 
is well suited for multiclass classification and, since 
the classes are integers and not one-hot encoded, the sparse version is preferred.~\cite{entropy}

\paragraph{Choosing an optimizer}
When choosing an optimizer for a neural network one must take into account the 
cost of reaching a minimum point on the error function.
Although more complex optimizers exist, build to reduce training 
cost or achieve better performances on deep networks, the one choosen 
for this model is a classic Stochastic Gradient Descent optimizer.

Testing some more advanced optimizers shows that convergence is reached faster on the 
networks used in the experiments, but the speed increase is negligible as the model
is quite small.

\subsection{Initial training set results}

Section \vref{feature-extraction} talks about the four different 
training sets obtained from the dataset and, without going into details, 
states that there is continuous improvement. 
We now give a more detailed look on training performances, after 
detailing how class imbalance is faced and the applied validation method.

Note that all the random seeds used by Tensorflow are fixed 
to make results reproducible. This step is necessary as many parameters initial 
value is random, for instance, the neural network weights.

\paragraph{Class imbalance}
To deal with the minority of some classes, balancing techniques should be 
applied when fitting the model. One of the possible approaches, and the one followed
here, is to assign class weights. 

The main idea is to penalize errors made on not well represented classes to account 
for their minority. Class weights computation relies on the \emph{compute class weights} function
from sklearn.~\cite{classweight}\\
The following are the computed quantities for the dataset classes:
\begin{center}
    \begin{tabular}{ |l|c|c| } 
        \hline
        Class name & Number of samples & Class weight \\
        \hline
        air conditioner & 500 & 0.8998 \\
        car horn & 208 & 2.1629 \\
        children playing & 500 & 0.8998 \\
        dog bark & 500 & 0.8998 \\
        drilling & 500 & 0.8998 \\
        engine idling & 517 & 0.8702 \\
        gun shot & 190 & 2.3678 \\
        jackhammer & 548 & 0.8209 \\
        siren & 536 & 0.8393 \\
        street music & 500 & 0.8998 \\
        \hline
    \end{tabular}
\end{center}
As expected, the less numerous classes have higher class weight than the rest,
in particular, the misclassification of a car horn sample
counts more than double than an air conditioner one.

\paragraph{Stratified cross-validation}
To estimate performance on the training set, stratified cross-validation with 
five folds is used. Basically the dataset is divided into five parts 
and a model is repeatedly trained on four and tested on one, all while considering class 
distribution, indeed, the original distribution of the classes is maintained 
in the splits.~\cite{stratified}

The stratified approach is required as there is class imbalance on the training set.
In fact, applying a classical cross-validation could show misleading results, 
for instance when the minority classes are more present 
in the test fold rather than the training ones; in such cases the loss would be 
higher.
The mean accuracy on the test folds gives a hint about the model performance.

For this step the \emph{Stratified KFold} class from scikit learn is used.~\cite{cross-scikit}

\paragraph{Results}
The following are the results on the training sets, using the architectures 
presented at the beginning of the Subsection.
\begin{center}
    \begin{tabular}{ |l|r|r| } 
        \hline
        Training set & Mean accuracy & St. deviation \\
        \hline
        132 features unscaled &  0.1138 & 0.0039 \\
        132 features scaled &  0.5743 & 0.0324 \\
        180 features scaled &  0.6363 & 0.0494 \\
        102 features reduced with PCA &  0.6188 & 0.0420 \\
        \hline
    \end{tabular}
\end{center}

There is a great improvement after scaling the training set, after 
that small refinements are made.
As accuracy is the best on the last two training sets, those are the two selected to 
perform the hyperparameter tuning.

\subsection{Hyperparameter tuning}

The last two tries with feature extraction lead to the best results 
with stratified cross-validation. Although the two models are reasonable, 
they can not be the final ones as many parameters are left on their default value, 
for instance, learning rate and momentum of the optimizer are untouched. 

The main goal now is to experiments with ranges of model parameters 
to find a better one. From now on, the model build on the 180 features training set is 
called \emph{Extended model}, 
while the one tested on the PCA training set is named \emph{PCA model}.

\paragraph{Grid and random search comparison}
Two of the most commonly used strategies in hyperparameter optimization
are \emph{grid} and \emph{random search}~\cite{random-grid}. 

In both cases we define ranges of parameters to test different combinations, 
for instance, fixed the number of neurons, one could try to find the best 
combination of learning rate and momentum that optimize accuracy on the training set.

While similar, the two methodologies differs in the amount of exploration they do.
The grid search try all the possible combinations of parameters, while the 
random approach fixes a number of iterations and picks an arbitrary unseen 
combination each time. 

Obviously the first one is more computationally expensive than the second, if 
we fix a small amount of possible iterations, but in theory it finds a better result
than going the random route. 
Nonetheless the grid search can led to over-fitting and in practice random 
search in preferred.

\paragraph{Random search}
We now run a random search with various parameters 
to optimize the initial models.
Note that class weights are still considered and the models are evaluated
again with a stratified cross-validation.
The optimizer used is the stochastic gradient descent.

The considered ranges for parameters for this run are: 
\begin{enumerate}
    \item \emph{Neurons}: input layer has dimension $H_i$ and last layer $H_o$, while 
    the two hidden layers are tested with a number of neurons respectively 
    equals to: 
    $$H_1 + 2i\;\;\text{and}\;\;H_2 + 2j,\;\;\text{with}\;\; i, j \in \{-2,-1,0,1,2\}$$
    \item \emph{Learning rate}: $0.001, 0.01, 0.1, 0.5$;
    \item \emph{Momentum}: $0.0, 0.01, 0.1, 1$;
    \item \emph{Epochs}: $60, 80, 100$;
    \item \emph{Batch size}: $32, 64$.
\end{enumerate}

Where $H_i$, $H_1$, $H_2$ and $H_o$ are input, first hidden, second hidden 
and output layer dimensions.
The random search is performed on two models, with those starting dimensions: 
\begin{itemize}
    \item \emph{Extended model}: 180, 80, 46, 10.
    \item \emph{PCA model}: 102, 45, 30 and 10;
\end{itemize}

An \emph{early stopper} with default parameters is used on the training to stop it when no progress is made with 
respect to the last epoch results.~\cite{early}
The search is performed with 100 iterations for both rounds.

\paragraph{Final models}
The first round of the random search, performed on the Extended model, resulted 
in the following parameters:
\begin{itemize}
    \item Neurons: 180 for input, 76 for the first hidden layer, 50 for the second and 10 for output;
    \item Momentum: 0.01
    \item Learning rate: 0.01
    \item Epochs: 80
    \item Batch size: 64
\end{itemize}
We can see an improvement in accuracy by comparing it with to starting point model:
\begin{center}
    \begin{tabular}{ |l|r|r| } 
        \hline
        Model & Mean accuracy & St. deviation \\
        \hline
        Initial extended model & 0.6363 & 0.0494\\
        Random search result & 0.6497 & 0.0431\\
        \hline
    \end{tabular}
\end{center}

The second random search on the smaller PCA model, resulted in this results:
\begin{itemize}
    \item Neurons: 102 for input, 45 for the first hidden layer, 30 for the second and 10 for output;
    \item Momentum: 0.01
    \item Learning rate: 0.1
    \item Epochs: 80
    \item Batch size: 32
\end{itemize}
As before, comparing it with the starting model, we can see some better results:
\begin{center}
    \begin{tabular}{ |l|r|r| } 
        \hline
        Model & Mean accuracy & St. deviation \\
        \hline
        Initial PCA model & 0.6188 & 0.0420\\
        Random search result & 0.6270 & 0.0315\\
        \hline
    \end{tabular}
\end{center}

\newpage