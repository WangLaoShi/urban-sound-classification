\section{Feature extraction}
\label{feature-extraction}

This Sections presents the original dataset structure and the 
steps followed to create training and test sets from it.

Note that the models mentioned in this section are
three layers networks with a reasonable number of neurons, trained 
for 100 epochs with default parameters for the Stochastic Gradient 
Descent optimizer. 
The accuracy on the training is computed with a cross validation 
approach.
Details about models and validation techniques used in 
the project can be found at Section \vref{model-definition}.

\subsection{Dataset structure}
\label{dataset-structure}

The dataset contains ten folds of audio samples, each one about 
four seconds long. The samples are divided in ten classes among ten 
folds. The following table shows the classes and their relative frequency 
in the dataset.

\begin{center}
    \begin{tabular}{ |l|c| } 
        \hline
        Class name & Number of samples \\
        \hline
        air conditioner & 1000 \\
        car horn & 429 \\
        children playing & 1000 \\
        dog bark & 1000 \\
        drilling & 1000 \\
        engine idling & 1000 \\
        gun shot & 374 \\
        jackhammer & 1000 \\
        siren & 929 \\
        street music & 1000 \\
        \hline
    \end{tabular}
\end{center}

The training set consists of the first four folds plus the sixth, 
the other folds create five different test sets.
The table shows the numerosity of the different sets.

\begin{center}
    \begin{tabular}{ |l|c| } 
        \hline
        Dataset & Number of samples \\
        \hline
        Training set & 4499 \\
        Test set 5 & 936 \\
        Test set 7 & 838 \\
        Test set 8 & 806 \\
        Test set 9 & 816 \\
        Test set 10 & 837 \\
        \hline
    \end{tabular}
\end{center}

\subsection{First dataset}
Choosing the features to extract was challenging due to the inexperience
with working on audio files, thus some research on what features to 
choose was necessary~\cite{features}.  

The Librosa library provides many feature to choose from,
to keep it simple, for the first try with this dataset, the extracted features 
are these three ones: 
\begin{enumerate}
    \item \emph{Mel-frequency cepstral coefficients}
    \item \emph{Chromagram}
    \item \emph{Root-mean-square}
\end{enumerate}
Each feature consists of an array of arrays containing measurements. 
A series of functions were applied to each sub-array and results 
were concatenated in a final feature vector~\cite{librosa-first}. 
The functions applied are \emph{minimum}, \emph{maximum}, \emph{mean} 
and \emph{median}.

This approach resulted in 132 components feature vectors.

\paragraph{Feature scaling}
After testing some neural networks on the first dataset the results 
were not promising. One of the reasons is the big difference in 
ranges among feature vector components.

To mitigate this effect a \emph{StandardScaler} from \emph{sklearn} was applied~\cite{scaler}\cite{scikitlearn}.
This lead to an improvement on the results using the same 
model as before. 

\subsection{Extended dataset}
\label{extended-dataset}

To improve results on the training set new features are added, namely: 
\begin{enumerate}
    \item \emph{Zero-crossing rate}
    \item \emph{Roll-off frequency}
    \item \emph{Spectral flux onset strength}
\end{enumerate}
After applying the same four functions to these three new arrays, 
a total of 12 new features are added to the dataset~\cite{librosa-ex}.
Scaling yield to promising results on the first dataset, 
so the same approach is applied to the extended dataset.

After testing a network on the new dataset results improved once again.

\paragraph{PCA}
Adding new features can lead to better results 
in the end but they all need to be useful to the model, so 
the extended dataset was subject of some experiments with feature selection, 
in particular \emph{PCA} algorithm from sklearn was applied~\cite{pca}.

After some experiments with the number of features to select, 120 out of 144 
features were selected. This led to a small improvement on the training set, 
so this final reduced dataset was selected to perform hyperparameter tuning.

\newpage