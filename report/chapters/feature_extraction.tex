\section{Feature extraction}
\label{feature-extraction}

This Sections presents the original dataset structure and the 
steps followed to create training and test sets from it.

Note that the models mentioned in this section are
three layers \emph{multilayer perceptron} with a reasonable number of neurons, trained 
for 100 epochs with default parameters for the \emph{Stochastic Gradient 
Descent optimizer}.~\cite{mlp}~\cite{sgd}

The accuracy on the training is computed with a \emph{cross-validation} 
approach.~\cite{cross}
Details about models and validation techniques used in 
the project can be found at Section \vref{model-definition}.

\subsection{Dataset structure}
\label{dataset-structure}

The dataset contains ten folds of audio samples, each one about 
four seconds long. The samples are divided in ten classes. 

From the total of ten folds, the number one, two, three, four and six 
are taken as a training set, the others are each one a test set.
For this reason the following count about class numerosity considers
only the five training folds.

\begin{center}
    \begin{tabular}{ |l|c| } 
        \hline
        Class name & Number of samples \\
        \hline
        air conditioner & 500 \\
        car horn & 208 \\
        children playing & 500 \\
        dog bark & 500 \\
        drilling & 500 \\
        engine idling & 517 \\
        gun shot & 190 \\
        jackhammer & 548 \\
        siren & 536 \\
        street music & 500 \\
        \hline
    \end{tabular}
\end{center}

The table shows a clear class imbalance. In particular, the classes 
\emph{car horn} and \emph{gun shot} are not as numerous as the others. 
This can lead to poor performances on the these two categories, it is
therefore taken 
into consideration with training. 

The following table shows the number of samples in the training set and 
the various test sets.

\begin{center}
    \begin{tabular}{ |l|c| } 
        \hline
        Dataset & Number of samples \\
        \hline
        Training set & 4499 \\
        Test set 5 & 936 \\
        Test set 7 & 838 \\
        Test set 8 & 806 \\
        Test set 9 & 816 \\
        Test set 10 & 837 \\
        \hline
    \end{tabular}
\end{center}

All the operations on the datasets are performed with \emph{Pandas} library.~\cite{pandas}

\subsection{First dataset}
Choosing the features to extract was challenging due to the inexperience working on audio files, thus some \emph{research} on what features to 
choose was necessary~\cite{features}.

To extract information from audio files \emph{Librosa} was used.~\cite{librosa}
The library provides many methods to choose from,
to keep it simple, for the first try with this dataset, the extracted features 
are these three ones: 
\begin{enumerate}
    \item \emph{Mel-frequency cepstral coefficients};
    \item \emph{Chromagram};
    \item \emph{Root-mean-square}.
\end{enumerate}
Each feature consists of an array of arrays containing measurements. 
A series of functions were applied to each sub-array and results 
were concatenated in a final feature vector. 
The functions applied are \emph{minimum}, \emph{maximum}, \emph{mean} 
and \emph{median} from the \emph{Numpy} library~\cite{numpy}.

This approach resulted in 132 components feature vectors.

\paragraph{Parallelizing feature extraction}
Extracting the three features listed above is really intensive 
but the task is easily parallelizable, in fact, each file is independent 
from one another.

For this purpose \emph{Dask} was used to speed up the computation, and 
extract features from audio files in a multi-processing fashion.~\cite{dask}

\paragraph{Feature scaling}
After testing some Neural Networks on the first dataset the results 
were not promising. One of the reasons is the big difference in 
ranges among feature vector components.

To mitigate this effect a \emph{StandardScaler} from \emph{scikit learn} was applied~\cite{scaler}.
The result is a dataset where each feature has more or less a distribution 
centered in zero with unit variance.
This lead to an improvement on the results using the same
model as before. 

\subsection{Extended dataset}
\label{extended-dataset}

Results using the three features named in the previous Subsection 
are promising but not enough, thus, to improve results on the training set, 
new audio characteristics are extracted, namely: 
\begin{enumerate}
    \item \emph{Zero-crossing rate};
    \item \emph{Roll-off frequency};
    \item \emph{Spectral flux onset strength}.
\end{enumerate}
As before, \emph{minimum}, \emph{maximum}, \emph{mean} 
and \emph{median} are applyied to each feature vector and 
results are concatenated, resulting in 12 new features for each 
audio file.
Scaling yield to promising results on the first dataset, 
so the same approach is applied to the extended one.

After testing a network on the new training set 
results improved once again.

\paragraph{Feature selection}
Adding new features can lead to better results 
in the end but they all need to be useful to the model, so 
the extended dataset was subject of some experiments with feature selection, 
in particular \emph{PCA} algorithm from scikit learn was applied~\cite{pca}.

The main idea is to select a reduced number of features from the total, 
without loosing information. This approach often leads to better results, 
as useless features are discarded.

After some experiments with the number of features to select, 120 out of 144
 were selected. This led to a small improvement on the training set.

\newpage